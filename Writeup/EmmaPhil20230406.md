## Models considered

Talk about the 5 models

## Data pre-processing

How do you define $d$ for the different models, talk about plumes and rifts.

## Representing the variation of a variable with distance using a spline

$y_j(d)$ is the value of variable $y_j$ ($j=1,2,...,n_j$) at distance $d$.

We estimate $y_j(d)$ using function $\mu_j(d)$ which is a linear combination of B-spline basis functions

$\mu_j(d) = \sum_k b_k(d) \beta_{kj}$

where $b_k(d)$ is the value of basis function $k$ at distance $d$, for $k=1,2,...,n_k$, and $d \in \mathcal{D}$, the domain of distance  $\mathcal{D}=[0,1800]$ km. The values of $b_k(d)$ for all $k$ and $d$ can be easily calculated, and they do not depend on the choice of variable $j$.

Spline functions of different order and complexity could be used. We use cubic B-splines, a popular choice with good general properties (***REF***), within a roughness penalisation framework: we specify a large number $n_k$ of basis functions on the distance domain, which means in principle we can a wide range of functions on $\mathcal{D}$, with very different roughnesses. We then need to select a curve with the optimal roughness to represent $\mu_j(d)$.

## Optimal roughness tuning

We need to estimate the values of $\beta_{jk}$ which gives best agreement between $y_j$ and $\mu_j$.

We could do this by minimising 

$\sum_{i : A(i) = \text{Trn}} (y_j(d_i)-\mu_j(d_i))^2$.

However, to keep the spline smooth, we actually minimise $\sum_{i : A(i) = \text{Trn}} (y_j(d_i)-\mu_j(d_i))^2 + \lambda \beta_j' P \beta_j$ where $P=D'D$, and $D$ is a difference matrix. ***Say what D is***

We choose the roughness penalty $\lambda_j$ to maximise predictive performance on a tuning set, by minimising the mean square error of prediction ($\text{MSEP}_j$)

$\text{MSEP}_j(\lambda)=\frac{1}{n_{\text{Tun}}} \sum_{i : A(i) = \text{Tun}} (y_j(d_i)-\mu_j(d_{i}|\lambda))^2$

## Estimating predictive performance

The value of $\text{MSEP}_j(\lambda_{j^*})$ is not a reliable estimate of the true predictive performance of the model, because the value of $\lambda_{j^*}$ was chosen using the tuning set of data to maximise performance. 

Therefore, we obtain a better estimate of the predictive performance using a test set, and estimating 

$\text{MSEP}_j=\frac{1}{n_{\text{Tst}}} \sum_{i : A(i) = \text{Tst}} (y_j(d_i)-\mu_j(d_{i}|\lambda_{j^*}))^2$

## Two-deep cross-validation

Explain how we partition data into $i : A(i) =$ "Train", or $i : A(i) =$ "Tune", or $i : A(i) =$ "Test". Just talk about how to define $A$, and repeating to make sure we have stability.

## Overall performance

$\text{MSEP}=\sum_j \text{MSEP}_j/\text{var}_j$

where $\text{var}_j$ is an estimate for the variance of variable $j$ 

$\text{var}_j = \frac{1}{n_{\text{Tst}}} \sum_{i : A(i) = \text{Tst}} (y_j(d_i)-\bar{y}_{\text{Tst} j})^2$

where $\bar{y}_{\text{Tst} j}$ is the mean of the test set for variable $j$

$\bar{y}_{\text{Tst} j} = \frac{1}{n_{\text{Tst}}} \sum_{i : A(i) = \text{Tst}} y_j(d_i)$

# Methodology - outline - linear regression

$\mu_j(d) = a_j + b_j d$ 

for unknown constants $a_j$ and $b_j$


